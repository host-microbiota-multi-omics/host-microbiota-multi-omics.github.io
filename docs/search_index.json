[["index.html", "Host-Microbiota Multi-Omics MSc Course at the University of Copenhagen Chapter 1 Introduction", " Host-Microbiota Multi-Omics MSc Course at the University of Copenhagen Ostaizka Aizpurua1 Antton Alberdi2 Last update: 2025-11-19 Chapter 1 Introduction Welcome to the Host-Microbiota Multi-Omics Course! This webbook contains the materials for the practicals of the course. computation (HPC). This section will guide you through the necessary steps to set up your environment, understand command-line basics, and get acquainted with Snakemake. University of Copenhagen, ostaizka.aizpurua@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ "],["getting-ready.html", "Chapter 2 Getting ready 2.1 Get Visual Studio Code ready 2.2 Create a SSH connection to Mjolnir 2.3 Open the remote explorer", " Chapter 2 Getting ready The first step to get started with the course is to set up your environment. This includes installing Visual Studio Code, establishing a SSH connection to the Mjolnir HPC cluster, and opening the remote explorer. 2.1 Get Visual Studio Code ready Visual Studio Code (VS Code) is a lightweight, cross-platform code editor developed by Microsoft. It’s free, open source, and supports a wide range of programming languages. Download Visual studio code: https://code.visualstudio.com/download Using the extension manager, install the following plugins: Python by Microsoft Snakemake Language by Snakemake Remote - SSH by Microsoft Remote Explorer by Microsoft 2.2 Create a SSH connection to Mjolnir Ensure you are connected to the KU VPN Menu: View &gt; Command Palette &gt; Remote-SSH: Connect to Host Add New SSH Host ssh mjolnirgate.unicph.domain Type password This should have created a SSH connection to Mjolnir, which will enable you to create a Remote Explorer. 2.3 Open the remote explorer Explorer Connect to remote &gt; Open Folder Use the following path: /maps/projects/course_2/people/{ku-id} "],["command-line-basics.html", "Chapter 3 Command line basics", " Chapter 3 Command line basics Before diving into high-performance computing and Snakemake, it’s essential to understand some command-line basics. The command line, also known as the terminal or shell, is a text-based interface that allows users to interact with their computer’s operating system. Here are some fundamental commands and concepts to get you started: Navigating directories: pwd: Print the current working directory. ls: List files and directories in the current directory. cd &lt;directory&gt;: Change to a specified directory. File operations: cp &lt;source&gt; &lt;destination&gt;: Copy files or directories. mv &lt;source&gt; &lt;destination&gt;: Move or rename files or directories. rm &lt;file&gt;: Remove files. mkdir &lt;directory&gt;: Create a new directory. Viewing file contents: cat &lt;file&gt;: Display the contents of a file. less &lt;file&gt;: View file contents one page at a time. Getting help: man &lt;command&gt;: Display the manual for a command. &lt;command&gt; --help: Show help information for a command. "],["high-performance-computation.html", "Chapter 4 High Performance Computation 4.1 Modules 4.2 Screen 4.3 SLURM", " Chapter 4 High Performance Computation High-Performance Computing (HPC) refers to the use of powerful computers and parallel processing techniques to solve complex computational problems that require significant processing power and memory. HPC systems, often referred to as supercomputers or clusters, are designed to perform large-scale computations at high speeds, enabling researchers and scientists to tackle problems that would be infeasible on standard computers. 4.1 Modules Modules are a way to manage and organise software packages on HPC systems. They allow users to easily load, unload, and switch between different software versions and configurations without interfering with each other. Modules help maintain a clean and organised environment, making it easier for users to access the software they need for their work. We will need to load two key modules to run our pipelines. If you have your own conda, consider deactivating it temporarily to ensure the exercises run smoothly. # Remove any previously loaded modules module purge # Check for desired modules module avail snakemake # Load the relevant modules module load snakemake/9.4.0 module load miniconda/py39_23.1 # List the loaded modules module list Use should see exactly this: Currently Loaded Modulefiles: 1) miniconda/py39_23.1 2) snakemake/9.4.0 4.2 Screen Screen is a simple tool that keeps your command-line session running on a server even if your internet drops or you close your laptop. Think of it like putting your terminal in a locker—you can step away and later unlock it to find everything exactly where you left it. It also lets you make several “tabs” (windows) inside one SSH login and switch between them. On HPC clusters where tasks can run for hours, starting them inside Screen keeps them safe and easy to rejoin. It’s lightweight and saves you from losing work. To create a screen session for the entire duration of the workshop: screen -S hmmo_screen To get out from the session, you must type ctrl+a d To re-enter the session, simply: screen -r hmmo_screen And to close the session, just type exit. 4.3 SLURM SLURM (Simple Linux Utility for Resource Management) is a widely used open-source job scheduler and workload manager for high-performance computing (HPC) clusters. It handles three main tasks: Resource allocation deciding which jobs run on which nodes. Job scheduling prioritising and queuing jobs. Job execution launching and monitoring jobs across compute nodes. Snakemake can submit workflow rules directly to SLURM. When a rule is executed this way, Snakemake doesn’t run the job itself; instead, it generates an sbatch command with the appropriate resource requests and submits it to SLURM. From Snakemake 8 onwards, it requires installation of the (executor plugin)[https://anaconda.org/bioconda/snakemake-executor-plugin-slurm]. 4.3.1 Job scheduler On Mjolnir, we use SLURM as a job scheduler to manage and allocate computational resources efficiently. SLURM allows users to submit jobs to a queue, where they wait for available resources before execution. This system helps ensure fair resource distribution among users and optimizes the overall performance of the cluster. When you submit a job using SLURM, you specify the resources you need (like CPU cores, memory, and time). SLURM then places your job in a queue and runs it when the requested resources become available. # Submit a job to the SLURM scheduler sbatch -J {job_name} --partition=cpuqueue --qos=normal --cpus-per-task=12 --mem=8G --time=5:00:00 --wrap=&quot;{command_to_run}&quot; # Shell file example cat &gt; job_to_launch.sh &lt;&lt; &#39;EOF&#39; #!/bin/bash #SBATCH -J example_job #SBATCH --partition=cpuqueue #SBATCH --qos=normal #SBATCH --cpus-per-task=12 #SBATCH --mem=8G #SBATCH --time=5:00:00 {command_to_run} EOF sbatch job_to_launch.sh # Check the status of your jobs in the queue squeue -u {ku-id} 4.3.2 Interactive job On a shared cluster like Mjolnir, the entry (login) nodes are the front door: they’re meant for quick, lightweight tasks like connecting, moving files, loading modules, editing scripts, and submitting jobs. They are shared by everyone and deliberately constrained (CPU, memory, long-running limits). Running heavy computations there slows the system for all users and can trigger automatic kills from cluster policies. An interactive session gives you a reserved slice of the compute nodes through the scheduler (e.g., Slurm). When you are in the entry node, the following command should return. hostname mjolnirgate01fl.unicph.domain In this workshop, each participant will get allocated 4 CPUs and 16GB of memory at node mjolnircomp14fl. When that time is over, the session will close automatically. srun \\ --job-name=snakemake_workshop \\ --account=teaching \\ --reservation=snakemake_hackaton \\ --partition=cpuqueue \\ --nodes=1 \\ --nodelist=mjolnircomp14fl \\ --time=08:00:00 \\ --mem=16G \\ --cpus-per-task=4 \\ --pty bash -I This will display for a few seconds, after which you will get connected to the computation node. srun: job XXXXXXXXX queued and waiting for resources To verify you are in the computing node the hostname command should now display: mjolnircomp14fl.unicph.domain To exit the interactive job, just type exit. "],["snakemake.html", "Chapter 5 Snakemake 5.1 The Snakefile 5.2 The rules 5.3 Linking rules together 5.4 Executing the pipeline", " Chapter 5 Snakemake Snakemake is a workflow management system that enables the creation of reproducible and scalable data analysis pipelines. It allows users to define complex workflows using a simple and intuitive syntax, making it easier to manage and automate data processing tasks. Snakemake is particularly popular in bioinformatics and computational biology, where it is used to handle large datasets and complex analyses. Snakemake is a rule-based workflow system. Each rule says: What it produces (the output) What it needs to produce that (the input) How to produce it (the command + parameters) Snakemake then figures out the right order to run everything and which parts can run at the same time. 5.1 The Snakefile The Snakefile is the heart of any Snakemake workflow. It is a plain text file—usually named Snakefile (with no file extension)—written in a mix of Snakemake’s workflow syntax and regular Python code. It serves as: A blueprint for your entire analysis pipeline A dependency map, describing what each step needs and produces An execution plan that Snakemake follows to run your tasks in the correct order The Snakefile is human-readable, version-controllable (e.g., with Git), and shareable. 5.2 The rules The Snakefile contains (at least) the rules that define the pipeline. 5.2.1 The ALL rule You typically start by specifying what you ultimately want to produce (plots, tables, reports) in the all rule: rule all: input: &quot;results/plot.png&quot; In this example, the pipeline will yield a plot.png file in the results directory. But you can also define multiple final outputs: rule all: input: &quot;results/plot.png&quot;, &quot;results/stats.tsv&quot; 5.2.2 A regular rule The regular rules define the path to reach the final outputs. Each rule at least must have a: Name: e.g., rule clean_data. Input: files or results needed. Output: files the step produces. Command: the command or script that performs the transformation. This rule, for example, outputs the row counts of a file to another file: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; 5.2.2.1 Name It must be unique, and ideally informative, as it will be used to identify the rules in the overview files, logs, etc. If you use repeated names, snakemake will complain and stop working. 5.2.2.2 Input and output Snakemake lets you describe input and output files in several convenient ways. 5.2.2.2.1 Single file rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; or rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; 5.2.2.2.2 Multiple unordered files rule count_lines: input: [&quot;results/a.counts.txt&quot;, &quot;results/b.counts.txt&quot;] output: &quot;results/merged.tsv&quot; shell: &quot;cat {input} &gt; {output}&quot; In this case, {input} expands to a space-separated list, this example resulting in: &quot;cat results/a.counts.txt results/b.counts.txt &gt; results/merged.tsv&quot; 5.2.2.2.3 Named children (recommended for clarity) rule count_lines: input: read1=&quot;reads/sample1_R1.fastq.gz&quot;, read2=&quot;reads/sample1_R2.fastq.gz&quot;, ref=&quot;refs/genome.fa&quot; output: bam=&quot;results/sample1.bam&quot; shell: &quot;bwa mem {input.ref} {input.read1} {input.read2} | &quot; &quot;samtools view -b -o {output.bam} 2&gt; {log.err}&quot; Use dot notation like {input.read1}, {output.bam}, etc. 5.2.2.3 Command Snakemake gives you several ways to express the command of a rule. 5.2.2.3.1 Shell The most common one is a regular shell command: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; The command definition can also be more complex, and expressed in multiple lines: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;&quot;&quot; coverm genome \\ -b {input} \\ -s _ \\ -m relative_abundance count \\ --min-covered-fraction 0 \\ &gt; {output} if [ $(stat -c &#39;%s&#39; {input}) -lt 1 ] then rm {input} fi &quot;&quot;&quot; 5.2.2.3.2 Script Or can run python scripts instead: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; script: &quot;scripts/normalize.py&quot; 5.2.2.3.3 Run Or an inline Python block: rule concat: input: [&quot;pieces/a.txt&quot;, &quot;pieces/b.txt&quot;] output: &quot;all.txt&quot; run: with open(output[0], &quot;w&quot;) as out: for p in input: out.write(open(p).read()) 5.2.2.4 Child arguments 5.3 Linking rules together One of Snakemake’s most powerful features is that you don’t need to manually tell it the order in which to run rules. Instead, Snakemake infers the correct sequence automatically by looking at: The output files of one rule The input files of another When the output of one rule matches the input of another, Snakemake creates a connection between them—this connection is a dependency. Let’s see an example: rule all: input: &quot;results/summary.txt&quot; rule preprocess: input: &quot;raw/data.csv&quot; output: &quot;processed/data_clean.csv&quot; shell: &quot;python scripts/clean_data.py {input} {output}&quot; rule summarize: input: &quot;processed/data_clean.csv&quot; output: &quot;results/summary.txt&quot; shell: &quot;python scripts/summarize.py {input} {output}&quot; 5.4 Executing the pipeline The execution of the pipeline is very straightforward. From the command line, in the directory containing your Snakefile, just run: snakemake By default, Snakemake looks for a file named Snakefile in the current directory, or in the workflow directory, both with and without a capital S. snakefile Snakefile workflow/snakefile workflow/Snakefile It is also possible to define a different name for the snakefile, usually with the extension .smk. If so, you need to tell snakemake where the snakefile is: snakemake -s mydirectory/mysnakefile.smk "],["course-environment.html", "Chapter 6 Course environment 6.1 Your directory 6.2 Raw data", " Chapter 6 Course environment The computational exercises of the course will be conducted in the following project in Mjolnir: /projects/course_1 You can access it using cd /projects/course_1 or visualise its contents using ls -lah /projects/course_1. This directory contains four folders: apps: it contains the snakemake pipelines you will use for processing the samples. data: it contains the raw data to be used in the exercises. people: it contains the folders of the individual users where your personal data will be stored. scratch: some software will use this directory to store temporary data. 6.1 Your directory This is the directory that can be accessed and edited by each user. All the computed files will be stored here. To access your directory you need to know your ku-id. /projects/course_1/people/{ku-id} 6.2 Raw data The raw data for the different studies that will be used in the course are found in different folders within the data director: /projects/course_1/data/lizards /projects/course_1/data/corvids "],["preprocessing.html", "Chapter 7 Preprocessing 7.1 Quality filtering 7.2 Host mapping 7.3 Host genome mapping 7.4 Mapping statistics 7.5 Host-microbiota split", " Chapter 7 Preprocessing 7.1 Quality filtering This rule performs the first major preprocessing step in the workflow: quality-control, trimming, and filtering of raw paired-end sequencing reads using fastp. It takes each sample’s raw FASTQ files and removes adapters, trims low-quality bases, filters out low-complexity or artifact-rich reads (including common NovaSeq poly-G/X issues), and outputs cleaned read pairs. These cleaned FASTQ files form the high-quality input for all downstream host–microbiota analyses, ensuring that later steps—such as host-read removal, taxonomic profiling, or assembly—are not biased by sequencing artifacts. In addition to generating cleaned reads, the rule produces HTML and JSON quality-control reports summarizing read quality before and after processing. The rule also handles software environment setup through module loading and scales memory/runtime requirements automatically based on the size of the input files, making it robust on HPC systems. rule fastp: input: r1=f&quot;{OUTPUT_DIR}/data/reads/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/data/reads/{{sample}}_2.fq.gz&quot; output: r1=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_2.fq.gz&quot;, html=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}.html&quot;, json=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}.json&quot; shell: &quot;&quot;&quot; module load {params.fastp_module} fastp \\ --in1 {input.r1} --in2 {input.r2} \\ --out1 {output.r1} --out2 {output.r2} \\ --trim_poly_g \\ --trim_poly_x \\ --low_complexity_filter \\ --n_base_limit 5 \\ --qualified_quality_phred 20 \\ --length_required 60 \\ --thread {threads} \\ --html {output.html} \\ --json {output.json} \\ --adapter_sequence AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\ --adapter_sequence_r2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT &quot;&quot;&quot; 7.2 Host mapping This rule prepares a reference genome so it can be efficiently used for read alignment with Bowtie2. It takes a FASTA file for a given host genome and runs bowtie2-build to generate the index files that Bowtie2 requires for fast and accurate mapping. Only one of the produced index files is listed as the output, but the command generates the full set of Bowtie2 index components. This indexing step is essential because alignment tools rely on these precomputed structures to rapidly search the genome during mapping. In addition to building the index, the rule also saves a copy of the reference FASTA alongside the index under a standardized naming scheme, ensuring consistency for downstream processes that expect the reference to be colocated with its index. The rule loads the appropriate Bowtie2 module and includes adaptive memory and runtime estimation based on the size of the reference genome, making it robust for small microbial genomes as well as larger host genomes. rule reference_index: input: f&quot;{OUTPUT_DIR}/data/references/{{reference}}.fna&quot; output: index=f&quot;{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2&quot; params: basename=f&quot;{OUTPUT_DIR}/data/references/{{reference}}&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} bowtie2-build {input} {params.basename} cat {input} &gt; {params.basename}.fna &quot;&quot;&quot; 7.3 Host genome mapping This rule maps each sample’s quality-filtered paired-end reads to its corresponding reference genome using Bowtie2, producing a sorted BAM file that is ready for downstream analyses such as host read removal, quantification, or coverage estimation. It uses the Bowtie2 index generated in the previous step and the cleaned FASTQ files from fastp as inputs. The mapping is performed with multiple threads for efficiency, and the output is streamed directly into samtools to convert the SAM alignment to BAM format and sort it in a single streamlined command. By linking each sample to its appropriate reference through SAMPLE_TO_REFERENCE, the rule flexibly supports host–microbiota workflows where different samples may require different reference genomes. Memory and runtime are scaled according to the size of the input reads, ensuring robust performance across varied dataset sizes. The result is a properly formatted, sorted BAM file that downstream tools can use without additional preprocessing. rule reference_map: input: index=lambda wildcards: expand( f&quot;{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2&quot;, reference=[SAMPLE_TO_REFERENCE[wildcards.sample]] ), r1=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_2.fq.gz&quot; output: f&quot;{OUTPUT_DIR}/preprocessing/bowtie2/{{sample}}.bam&quot; params: basename=lambda wildcards: f&quot;{OUTPUT_DIR}/data/references/{SAMPLE_TO_REFERENCE[wildcards.sample]}&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} {params.samtools_module} bowtie2 -x {params.basename} -1 {input.r1} -2 {input.r2} -p {threads} | samtools view -bS - | samtools sort -o {output} &quot;&quot;&quot; 7.4 Mapping statistics This rule computes alignment quality and summary statistics for each sorted BAM file generated during mapping. Using several samtools subcommands, it produces metrics that describe how well the reads aligned to the reference genome—information essential for evaluating host-read removal efficiency, assessing sequencing quality, and feeding into MultiQC reports. The BAM file is indexed to enable rapid random access, while flagstat, idxstats, and stats each provide different levels of summary: overall alignment rates, per-reference sequence counts, and more detailed alignment metrics. All these outputs are written into a dedicated samtools directory and follow a consistent naming structure, making them easy to collect later for QC visualization and downstream processing. The rule loads the necessary samtools module and uses light computational resources, since generating these QC summaries is relatively inexpensive compared to alignment. rule samtools_stats: input: rules.reference_map.output output: bai = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.bam.bai&quot;, flagstat = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.flagstat.txt&quot;, idxstats = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.idxstats.txt&quot;, stats = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.stats.txt&quot; shell: &quot;&quot;&quot; module load {params.samtools_module} samtools index {input} {output.bai} samtools flagstat {input} &gt; {output.flagstat} samtools idxstats {input} &gt; {output.idxstats} samtools stats {input} &gt; {output.stats} &quot;&quot;&quot; 7.5 Host-microbiota split This rule takes the aligned BAM file and separates mapped and unmapped read pairs, effectively splitting host-origin reads from metagenomic reads. Unmapped reads (i.e., those that did not align to the host reference) are extracted and converted back into paired FASTQ files, forming the metagenomic read set used for downstream microbiome profiling or assembly. At the same time, mapped reads are retained in a host-only BAM file, which can be used for host genomics analyses or simply for quality assessment. In addition to generating the split read files, the rule also counts how many reads and how many total bases fall into the metagenomic vs. host categories. These summary files are useful for evaluating host contamination, sequencing depth distribution, and the overall success of the host-depletion strategy. The process relies on samtools to filter BAM records by mapping flags and uses simple awk commands to tally base counts, making it a compact but crucial step in workflows that jointly analyze host and microbiota data. rule split_reads: input: f&quot;{OUTPUT_DIR}/preprocessing/bowtie2/{{sample}}.bam&quot; output: r1=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz&quot;, metareads=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.metareads&quot;, metabases=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.metabases&quot;, bam=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam&quot;, hostreads=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.hostreads&quot;, hostbases=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.hostbases&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} {params.samtools_module} samtools view -b -f12 -@ {threads} {input} | samtools fastq -@ {threads} -1 {output.r1} -2 {output.r2} - samtools view -b -f12 -@ {threads} {input} | samtools view -c - &gt; {output.metareads} samtools view -f12 -@ {threads} {input} | awk &#39;{{sum += length($10)}} END {{print sum}}&#39; &gt; {output.metabases} samtools view -b -F12 -@ {threads} {input} | samtools sort -@ {threads} -o {output.bam} - samtools view -b -F12 -@ {threads} {input} | samtools view -c - &gt; {output.hostreads} samtools view -F12 -@ {threads} {input} | awk &#39;{{sum += length($10)}} END {{print sum}}&#39; &gt; {output.hostbases} &quot;&quot;&quot; "],["host-genome.html", "Host genome 7.6 Sketching", " Host genome 7.6 Sketching "],["microbial-metagenome.html", "Microbial metagenome 7.7 Assembly 7.8 Assembly index 7.9 Assembly mapping 7.10 Assembly map depth 7.11 Binning 7.12 Bin refinement 7.13 Rename bins 7.14 Functional annotation 7.15 Dereplication", " Microbial metagenome 7.7 Assembly This rule performs metagenomic assembly using MEGAHIT. For each defined assembly group, it gathers the metagenomic paired-end reads (R1 and R2) from one or more samples, combines them, and runs MEGAHIT to reconstruct longer contiguous sequences (contigs) from the short reads. This can be a single-sample assembly or a co-assembly of multiple related samples, depending on how ASSEMBLY_TO_SAMPLES is defined. The result is a FASTA file containing assembled contigs that represent fragments of microbial genomes present in the samples, filtered to retain only contigs above a certain minimum length to focus on more reliable sequences. The rule ensures that the output directory is clean before each run, uses multiple threads and dynamically allocated memory to handle larger datasets, and then standardizes the output by renaming MEGAHIT’s final contig file to a consistent path and filename. This assembled contig file becomes the basis for downstream cataloging steps such as binning, annotation, and construction of metagenome-assembled genomes (MAGs) in the host–microbiota multi-omics workflow. rule assembly: input: r1=lambda wildcards: [f&quot;{OUTPUT_DIR}/preprocessing/final/{sample}_1.fq.gz&quot; for sample in ASSEMBLY_TO_SAMPLES[wildcards.assembly]], r2=lambda wildcards: [f&quot;{OUTPUT_DIR}/preprocessing/final/{sample}_2.fq.gz&quot; for sample in ASSEMBLY_TO_SAMPLES[wildcards.assembly]] shell: &quot;&quot;&quot; rm -rf {params.outputdir} # Convert input list to a comma-separated string R1_FILES=$(echo {input.r1} | tr &#39; &#39; &#39;,&#39;) R2_FILES=$(echo {input.r2} | tr &#39; &#39; &#39;,&#39;) megahit \\ -t {threads} \\ --verbose \\ --min-contig-len 1500 \\ -1 $R1_FILES -2 $R2_FILES \\ -o {params.outputdir} mv {params.outputdir}/final.contigs.fa {output} &quot;&quot;&quot; 7.8 Assembly index This rule creates a Bowtie2 index for each metagenomic assembly generated in the previous step. It takes the assembled contigs (the reconstructed metagenome) and runs bowtie2-build to generate the indexing files required for efficient read alignment. Indexing metagenomic assemblies is essential for downstream processes such as read recruitment, contig coverage estimation, binning, and quality assessment of assembled genomes. The rule ensures the index is stored alongside the assembly under a consistent naming scheme, allowing later rules to easily locate it. It uses modest computational resources, since indexing contigs is generally lightweight compared to assembly, and loads the correct Bowtie2 module to guarantee reproducibility. This step prepares the assembled contigs to be used as a searchable reference in subsequent cataloging analyses. rule assembly_index: input: f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.fna&quot; output: index=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.rev.2.bt2&quot; params: bowtie2_module={BOWTIE2_MODULE}, basename=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}&quot; shell: &quot;&quot;&quot; bowtie2-build {input} {params.basename} &quot;&quot;&quot; 7.9 Assembly mapping This rule maps each sample’s metagenomic reads back to a specific metagenomic assembly using Bowtie2. By aligning the cleaned, host-filtered reads to the assembled contigs, the workflow can quantify how much support each contig has across samples—information that is essential for downstream steps such as binning, abundance estimation, contamination checking, and cross-sample comparative analyses. The rule uses the Bowtie2 index generated for the assembly and the final FASTQ files produced during host–microbiota read separation. The alignment output is directly converted to a sorted BAM file through samtools, ensuring the result is immediately usable for coverage calculation or other depth-based analyses. With multi-threading support and adaptive resource allocation, the rule efficiently handles both small and large datasets. This mapping step links reads back to the assembly, turning raw contigs into quantitatively interpretable components of the microbial community. rule assembly_map: input: index=lambda wildcards: f&quot;{OUTPUT_DIR}/cataloging/megahit/{wildcards.assembly}/{wildcards.assembly}.rev.2.bt2&quot;, r1=lambda wildcards: f&quot;{OUTPUT_DIR}/preprocessing/final/{wildcards.sample}_1.fq.gz&quot;, r2=lambda wildcards: f&quot;{OUTPUT_DIR}/preprocessing/final/{wildcards.sample}_2.fq.gz&quot; output: f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{{assembly}}/{{sample}}.bam&quot; params: basename=lambda wildcards: f&quot;{OUTPUT_DIR}/cataloging/megahit/{wildcards.assembly}/{wildcards.assembly}&quot; shell: &quot;&quot;&quot; bowtie2 -x {params.basename} -1 {input.r1} -2 {input.r2} | samtools view -bS - | samtools sort -o {output} &quot;&quot;&quot; 7.10 Assembly map depth This rule computes per-contig read depth across all samples associated with a given assembly. It takes the set of BAM files produced by mapping each sample’s metagenomic reads back to the assembly and uses jgi_summarize_bam_contig_depths (from MetaBAT2) to generate a depth table. This table reports how many reads align to each contig in each sample—information that is essential for binning tools, which rely on differential coverage patterns to group contigs into metagenome-assembled genomes (MAGs). The rule outputs two versions of the depth file: the full MetaBAT2-compatible depth matrix and a simplified depth file formatted for MaxBin2. These coverage profiles are a key input for downstream binning workflows, as they allow algorithms to distinguish which contigs likely originate from the same genome. rule assembly_map_depth: input: lambda wildcards: [ f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{wildcards.assembly}/{sample}.bam&quot; for sample in ASSEMBLY_TO_SAMPLES[wildcards.assembly] ] output: metabat2=f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{{assembly}}_metabat.depth&quot;, maxbin2=f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{{assembly}}_maxbin.depth&quot; shell: &quot;&quot;&quot; jgi_summarize_bam_contig_depths --outputDepth {output.metabat2} {input} cut -f1,3 {output.metabat2} | tail -n+2 &gt; {output.maxbin2} &quot;&quot;&quot; 7.11 Binning MetaBAT2, MaxBin2, and SemiBin2 are complementary metagenomic binning tools that take assembled contigs and coverage information and attempt to cluster contigs into metagenome-assembled genomes (MAGs). Because no single algorithm is perfect for all datasets, using multiple binners increases the robustness and completeness of the final genome catalog. Each tool uses different combinations of signals—coverage patterns across samples, sequence composition, and marker-gene information—to infer which contigs originate from the same microbial genome. Running all three methods, then harmonizing the outputs, provides a stronger basis for downstream bin refinement and MAG quality filtering. 7.11.1 MetaBat2 MetaBAT2 clusters contigs based mainly on differential coverage across samples and tetranucleotide frequency, producing a TSV mapping contigs to bins. It is fast, widely used, and effective when multiple samples provide rich coverage variation. rule metabat2: input: assembly=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.fna&quot;, depth=f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{{assembly}}_metabat.depth&quot; output: f&quot;{OUTPUT_DIR}/cataloging/metabat2/{{assembly}}/{{assembly}}.tsv&quot; shell: &quot;&quot;&quot; metabat2 -i {input.assembly} -a {input.depth} -o {output} -m 1500 --saveCls --noBinOut &quot;&quot;&quot; 7.11.2 MaxBin2 MaxBin2 uses marker-gene detection, sequence composition, and abundance patterns to group contigs. It tends to perform well for genomes with typical marker sets, and the workflow includes logic to skip running it on very small assemblies where meaningful binning is unlikely. rule maxbin2: input: assembly=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.fna&quot;, depth=f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{{assembly}}_maxbin.depth&quot; output: f&quot;{OUTPUT_DIR}/cataloging/maxbin2/{{assembly}}/{{assembly}}.summary&quot; params: basename=f&quot;{OUTPUT_DIR}/cataloging/maxbin2/{{assembly}}/{{assembly}}&quot;, assembly_size_mb=lambda wildcards, input: int(Path(input.assembly).stat().st_size / (1024*1024)) shell: &quot;&quot;&quot; if (( {params.assembly_size_mb} &lt; 10 )); then echo &quot;Assembly is smaller than 10 MB, skipping maxbin2...&quot; touch {output} else MODULEPATH=/opt/shared_software/shared_envmodules/modules:$MODULEPATH \\ module load {params.maxbin2_module} {params.hmmer_module} rm -rf {params.basename}* run_MaxBin.pl -contig {input.assembly} -abund {input.depth} -max_iteration 10 -out {params.basename} -min_contig_length 1500 fi &quot;&quot;&quot; 7.11.3 SemiBin2 SemiBin2 performs machine-learning–based genome binning using the assembly together with all sample-specific BAM files. It integrates sequence composition, depth information, and neural-network–derived embeddings to cluster contigs into high-quality genome bins. This rule runs SemiBin2 in its “single easy bin” mode, which automates preprocessing steps such as marker gene detection and coverage calculation. rule semibin2: input: assembly=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.fna&quot;, bam=lambda wildcards: [ f&quot;{OUTPUT_DIR}/cataloging/bowtie2/{wildcards.assembly}/{sample}.bam&quot; for sample in ASSEMBLY_TO_SAMPLES[wildcards.assembly] ] output: f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}/contig_bins.tsv&quot; params: semibin2_module={SEMIBIN2_MODULE}, hmmer_module={HMMER_MODULE}, bedtools_module={BEDTOOLS_MODULE}, outdir=f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}&quot;, assembly_size_mb=lambda wildcards, input: int(Path(input.assembly).stat().st_size / (1024*1024)) threads: 8 resources: mem_mb=lambda wildcards, input, attempt: min(1000*1024,max(8*1024, int(input.size_mb * 30) * 2 ** (attempt - 1))), runtime=lambda wildcards, input, attempt: min(20000,max(15, int(input.size_mb / 2) * 2 ** (attempt - 1))) message: &quot;Binning contigs from assembly {wildcards.assembly} using semibin2...&quot; shell: &quot;&quot;&quot; if (( {params.assembly_size_mb} &lt; 10 )); then echo &quot;Assembly is smaller than 10 MB, skipping semibin2...&quot; touch {output} else module load {params.semibin2_module} {params.bedtools_module} {params.hmmer_module} SemiBin2 single_easy_bin -i {input.assembly} -b {input.bam} -o {params.outdir} -m 1500 -t {threads} --compression none fi &quot;&quot;&quot; This rule simply reformats its existing contig_bins.tsv output into a consistent table for integration with the other binners’ results. rule semibin2_table: input: f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}/contig_bins.tsv&quot; output: f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}/{{assembly}}.tsv&quot; params: fastadir=f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}/output_bins&quot; shell: &quot;&quot;&quot; tail -n +2 {input} &gt; {output} &quot;&quot;&quot; 7.12 Bin refinement This checkpoint performs bin refinement and quality assessment by combining the results from the three binners (MetaBAT2, MaxBin2, SemiBin2) using binette. It takes each binner’s contig-to-bin TSV plus the original assembly FASTA and feeds only the non-empty TSV files into binette. Binette then reconciles overlapping or conflicting bin assignments across tools, refines bin boundaries, and evaluates the resulting bins’ completeness and contamination (via CheckM2), producing a final table of bins with quality metrics. This step is where the workflow turns three separate binning runs into a unified, curated set of metagenome-assembled genomes. The shell logic first checks which of the individual binner outputs actually contain data (for example, small assemblies may have caused some tools to be skipped), builds a list of valid TSV files, and aborts with an error if none are available. If at least one binner produced bins, binette is run with those inputs and the contig FASTA, writing its results into a dedicated output directory and summarizing the final refined bins in final_bins_quality_reports.tsv. checkpoint binette: input: metabat2=f&quot;{OUTPUT_DIR}/cataloging/metabat2/{{assembly}}/{{assembly}}.tsv&quot;, maxbin2=f&quot;{OUTPUT_DIR}/cataloging/maxbin2/{{assembly}}/{{assembly}}.tsv&quot;, semibin2=f&quot;{OUTPUT_DIR}/cataloging/semibin2/{{assembly}}/{{assembly}}.tsv&quot;, fasta=f&quot;{OUTPUT_DIR}/cataloging/megahit/{{assembly}}/{{assembly}}.fna&quot; output: f&quot;{OUTPUT_DIR}/cataloging/binette/{{assembly}}/final_bins_quality_reports.tsv&quot; params: outdir=f&quot;{OUTPUT_DIR}/cataloging/binette/{{assembly}}&quot; shell: &quot;&quot;&quot; # Define input files METABAT2=&quot;{input.metabat2}&quot; MAXBIN2=&quot;{input.maxbin2}&quot; SEMIBIN2=&quot;{input.semibin2}&quot; # Remove empty input files from the list VALID_TSV_FILES=&quot;&quot; if [ -s &quot;$METABAT2&quot; ]; then VALID_TSV_FILES=&quot;$VALID_TSV_FILES $METABAT2&quot; fi if [ -s &quot;$MAXBIN2&quot; ]; then VALID_TSV_FILES=&quot;$VALID_TSV_FILES $MAXBIN2&quot; fi if [ -s &quot;$SEMIBIN2&quot; ]; then VALID_TSV_FILES=&quot;$VALID_TSV_FILES $SEMIBIN2&quot; fi # Ensure at least one valid TSV file exists if [ -z &quot;$VALID_TSV_FILES&quot; ]; then echo &quot;Error: No valid TSV input files for binette.&quot; &gt;&amp;2 exit 1 fi # Run binette only with non-empty TSV files binette --contig2bin_tables $VALID_TSV_FILES \\ --contigs {input.fasta} \\ --outdir {params.outdir} \\ --checkm2_db {params.checkm_db} \\ --threads {threads} &quot;&quot;&quot; 7.13 Rename bins This code connects the bin refinement results from Binette to the final, nicely named bin FASTA files that the user will see. The helper function get_bin_fna_sep uses the Binette checkpoint output to (internally) regenerate the list of bin IDs that exist for a given assembly. Conceptually, it tells Snakemake: “for this assembly, there are bins with these IDs, and their sequences live in final_bins/bin_.fa.” That way, the workflow can dynamically adapt to however many bins Binette produced without hard-coding bin names or counts. The rename_bins rule then takes each of those Binette bin FASTA files and runs a small Python script (rename_bins.py) to copy/rename them into a standardized, final location: final/{assembly}/{assembly}bin{bin_id}.fa. This step cleans up Binette’s internal naming scheme and produces consistently named MAG files that are easier to track, share, and feed into downstream analyses such as annotation or taxonomic classification. # Regenerate the bin_id wildcard based on the checkpoint results def get_bin_fna_sep(wildcards): checkpoint_output = checkpoints.binette.get(**wildcards).output[0] cluster_ids = get_bin_ids_from_tsv(checkpoint_output) return f&quot;{OUTPUT_DIR}/cataloging/binette/{{assembly}}/final_bins/bin_{wildcards.bin_id}.fa&quot; rule rename_bins: input: lambda wildcards: get_bin_fna_sep(wildcards) output: f&quot;{OUTPUT_DIR}/cataloging/final/{{assembly}}/{{assembly}}_bin_{{bin_id}}.fa&quot; params: package_dir={PACKAGE_DIR} shell: &quot;&quot;&quot; python {params.package_dir}/workflow/scripts/rename_bins.py {wildcards.assembly} {input} {output} &quot;&quot;&quot; 7.14 Functional annotation 7.15 Dereplication "],["metatranscriptome.html", "Meta(transcriptome)", " Meta(transcriptome) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
