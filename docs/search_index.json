[["index.html", "Host-Microbiota Multi-Omics MSc Course at the University of Copenhagen Chapter 1 Introduction", " Host-Microbiota Multi-Omics MSc Course at the University of Copenhagen Ostaizka Aizpurua1 Antton Alberdi2 Last update: 2025-12-09 Chapter 1 Introduction Welcome to the Host-Microbiota Multi-Omics Course! This webbook contains the materials for the practicals of the course. Each practical is organized in a separate chapter, which can be accessed from the sidebar on the left. The materials include step-by-step instructions, code snippets, and explanations to help you understand the concepts and techniques covered in the course. Note that these scripts are not meant to be run all at once. Instead, please follow the instructions in each chapter to run the code in the appropriate sections. Feel free to modify the code and experiment with different parameters to see how they affect the results. computation (HPC). This section will guide you through the necessary steps to set up your environment, understand command-line basics, and get acquainted with Snakemake. University of Copenhagen, ostaizka.aizpurua@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ "],["getting-ready.html", "Chapter 2 Getting ready 2.1 Get Visual Studio Code ready 2.2 Create a SSH connection to Mjolnir 2.3 Open the remote explorer", " Chapter 2 Getting ready The first step to get started with the course is to set up your environment. This includes installing Visual Studio Code, establishing a SSH connection to the Mjolnir HPC cluster, and opening the remote explorer. 2.1 Get Visual Studio Code ready Visual Studio Code (VS Code) is a lightweight, cross-platform code editor developed by Microsoft. It’s free, open source, and supports a wide range of programming languages. Download Visual studio code: https://code.visualstudio.com/download Using the extension manager, install the following plugins: Python by Microsoft Snakemake Language by Snakemake Remote - SSH by Microsoft Remote Explorer by Microsoft 2.2 Create a SSH connection to Mjolnir Ensure you are connected to the KU VPN Menu: View &gt; Command Palette &gt; Remote-SSH: Connect to Host Add New SSH Host ssh mjolnirgate.unicph.domain Type password This should have created a SSH connection to Mjolnir, which will enable you to create a Remote Explorer. 2.3 Open the remote explorer Explorer Connect to remote &gt; Open Folder Use the following path: /maps/projects/course_1/people/{ku-id} "],["command-line-basics.html", "Chapter 3 Command line basics 3.1 Navigating Directories 3.2 Working With Files and Directories 3.3 Viewing File Contents 3.4 File and Directory Information 3.5 Searching for Files and Text 3.6 Useful Beginner Tips", " Chapter 3 Command line basics Before diving into high-performance computing and Snakemake, it’s essential to understand some command-line basics. The command line, also known as the terminal or shell, is a text-based interface that allows users to interact with their computer’s operating system. Here are some fundamental commands and concepts to get you started: 3.1 Navigating Directories Moving around the filesystem efficiently is the first step to feeling comfortable at the command line. The commands below help you check where you are, list what’s around you, and jump to the folder you need. pwd — Print Working Directory Shows your current location in the filesystem. Example: $ pwd /home/user/projects ls — List files and directories Useful options: ls -l — long format (permissions, sizes, timestamps) ls -a — show hidden files Example: $ ls -la drwxr-xr-x 5 user user 4096 Feb 10 project1 -rw-r--r-- 1 user user 512 Feb 10 notes.txt cd &lt;directory&gt; — Change directory Examples: cd project1 cd .. # go up one level cd ~ # go to home directory cd /path/to/folder 3.2 Working With Files and Directories Once you can move around, you’ll want to organize what you find. These commands cover copying, moving, deleting, and creating files and folders so you can keep your workspace tidy. cp &lt;source&gt; &lt;destination&gt; — Copy files or folders Add -r to copy directories recursively. Examples: cp file.txt backup.txt cp -r data/ backup_data/ mv &lt;source&gt; &lt;destination&gt; — Move or rename files Examples: mv old_name.txt new_name.txt mv file.txt /home/user/documents/ rm &lt;file&gt; — Remove files ⚠️ Removal is permanent — use with caution. Add -r to remove directories. Examples: rm old_file.txt rm -r old_folder/ mkdir &lt;directory&gt; — Create new directories Examples: mkdir results mkdir -p path/to/new/folder # creates nested folders 3.3 Viewing File Contents When you just need to inspect or skim a file, these options let you read it right from the terminal without opening an editor. cat &lt;file&gt; — Print entire file cat sample.txt less &lt;file&gt; — View large files page-by-page Useful shortcuts: q — quit ↑/↓ — scroll /word — search less longfile.log head &lt;file&gt; — Show the first 10 lines head data.txt tail &lt;file&gt; — Show the last 10 lines tail data.txt tail -f logfile.log # follow live updates 3.4 File and Directory Information Disk quotas matter on shared systems. Use these commands to see how much space your files use and how much is left on the system. du — Disk usage summary du -sh folder/ df -h — Show available disk space df -h 3.5 Searching for Files and Text Finding the right file or line quickly saves time. These tools search filenames and file contents directly from the command line. grep &lt;pattern&gt; &lt;file&gt; — Search text in files grep &quot;ERROR&quot; logfile.log grep -i &quot;gene&quot; annotations.txt # case-insensitive grep -r &quot;pattern&quot; ./data # search recursively find &lt;path&gt; -name &lt;pattern&gt; — Search for files find . -name &quot;*.txt&quot; 3.6 Useful Beginner Tips A few small habits make terminal work faster and less error-prone. Try these shortcuts and helpers as you practice. Press Tab to autocomplete commands and filenames. Use ↑/↓ to browse command history. Run history to see past commands. Add comments in the terminal with #: ls -la # list files in detail including hidden ones "],["high-performance-computation.html", "Chapter 4 High Performance Computation 4.1 Modules 4.2 Screen 4.3 SLURM", " Chapter 4 High Performance Computation High-Performance Computing (HPC) refers to the use of powerful computers and parallel processing techniques to solve complex computational problems that require significant processing power and memory. HPC systems, often referred to as supercomputers or clusters, are designed to perform large-scale computations at high speeds, enabling researchers and scientists to tackle problems that would be infeasible on standard computers. 4.1 Modules Modules are a way to manage and organise software packages on HPC systems. They allow users to easily load, unload, and switch between different software versions and configurations without interfering with each other. Modules help maintain a clean and organised environment, making it easier for users to access the software they need for their work. We will need to load two key modules to run our pipelines. If you have your own conda, consider deactivating it temporarily to ensure the exercises run smoothly. # Remove any previously loaded modules module purge # Check for desired modules module avail snakemake # Load the relevant modules module load snakemake/9.4.0 module load miniconda/py39_23.1 # List the loaded modules module list Use should see exactly this: Currently Loaded Modulefiles: 1) miniconda/py39_23.1 2) snakemake/9.4.0 4.2 Screen Screen is a simple tool that keeps your command-line session running on a server even if your internet drops or you close your laptop. Think of it like putting your terminal in a locker—you can step away and later unlock it to find everything exactly where you left it. It also lets you make several “tabs” (windows) inside one SSH login and switch between them. On HPC clusters where tasks can run for hours, starting them inside Screen keeps them safe and easy to rejoin. It’s lightweight and saves you from losing work. To create a screen session for the entire duration of the workshop: screen -S hmmo_screen To get out from the session, you must type ctrl+a d To re-enter the session, simply: screen -r hmmo_screen And to close the session, just type exit. 4.3 SLURM SLURM (Simple Linux Utility for Resource Management) is a widely used open-source job scheduler and workload manager for high-performance computing (HPC) clusters. It handles three main tasks: Resource allocation deciding which jobs run on which nodes. Job scheduling prioritising and queuing jobs. Job execution launching and monitoring jobs across compute nodes. Snakemake can submit workflow rules directly to SLURM. When a rule is executed this way, Snakemake doesn’t run the job itself; instead, it generates an sbatch command with the appropriate resource requests and submits it to SLURM. From Snakemake 8 onwards, it requires installation of the (executor plugin)[https://anaconda.org/bioconda/snakemake-executor-plugin-slurm]. 4.3.1 Job scheduler On Mjolnir, we use SLURM as a job scheduler to manage and allocate computational resources efficiently. SLURM allows users to submit jobs to a queue, where they wait for available resources before execution. This system helps ensure fair resource distribution among users and optimizes the overall performance of the cluster. When you submit a job using SLURM, you specify the resources you need (like CPU cores, memory, and time). SLURM then places your job in a queue and runs it when the requested resources become available. # Submit a job to the SLURM scheduler sbatch -J {job_name} --partition=cpuqueue --qos=normal --cpus-per-task=12 --mem=8G --time=5:00:00 --wrap=&quot;{command_to_run}&quot; # Shell file example cat &gt; job_to_launch.sh &lt;&lt; &#39;EOF&#39; #!/bin/bash #SBATCH -J example_job #SBATCH --partition=cpuqueue #SBATCH --qos=normal #SBATCH --cpus-per-task=12 #SBATCH --mem=8G #SBATCH --time=5:00:00 {command_to_run} EOF sbatch job_to_launch.sh # Check the status of your jobs in the queue squeue -u {ku-id} 4.3.2 Interactive job On a shared cluster like Mjolnir, the entry (login) nodes are the front door: they’re meant for quick, lightweight tasks like connecting, moving files, loading modules, editing scripts, and submitting jobs. They are shared by everyone and deliberately constrained (CPU, memory, long-running limits). Running heavy computations there slows the system for all users and can trigger automatic kills from cluster policies. An interactive session gives you a reserved slice of the compute nodes through the scheduler (e.g., Slurm). When you are in the entry node, the following command should return. hostname mjolnirgate01fl.unicph.domain In this workshop, each participant will get allocated 4 CPUs and 16GB of memory at node mjolnircomp14fl. When that time is over, the session will close automatically. srun \\ --job-name=hmmo \\ --nodes=1 \\ --time=08:00:00 \\ --mem=16G \\ --ntasks=4 \\ --pty bash This will display for a few seconds, after which you will get connected to the computation node. srun: job XXXXXXXXX queued and waiting for resources To verify you are in the computing node the hostname command should now display: mjolnircomp14fl.unicph.domain To exit the interactive job, just type exit. "],["snakemake.html", "Chapter 5 Snakemake 5.1 The Snakefile 5.2 The rules 5.3 Linking rules together 5.4 Executing the pipeline", " Chapter 5 Snakemake Snakemake is a workflow management system that enables the creation of reproducible and scalable data analysis pipelines. It allows users to define complex workflows using a simple and intuitive syntax, making it easier to manage and automate data processing tasks. Snakemake is particularly popular in bioinformatics and computational biology, where it is used to handle large datasets and complex analyses. Snakemake is a rule-based workflow system. Each rule says: What it produces (the output) What it needs to produce that (the input) How to produce it (the command + parameters) Snakemake then figures out the right order to run everything and which parts can run at the same time. 5.1 The Snakefile The Snakefile is the heart of any Snakemake workflow. It is a plain text file—usually named Snakefile (with no file extension)—written in a mix of Snakemake’s workflow syntax and regular Python code. It serves as: A blueprint for your entire analysis pipeline A dependency map, describing what each step needs and produces An execution plan that Snakemake follows to run your tasks in the correct order The Snakefile is human-readable, version-controllable (e.g., with Git), and shareable. 5.2 The rules The Snakefile contains (at least) the rules that define the pipeline. 5.2.1 The ALL rule You typically start by specifying what you ultimately want to produce (plots, tables, reports) in the all rule: rule all: input: &quot;results/plot.png&quot; In this example, the pipeline will yield a plot.png file in the results directory. But you can also define multiple final outputs: rule all: input: &quot;results/plot.png&quot;, &quot;results/stats.tsv&quot; 5.2.2 A regular rule The regular rules define the path to reach the final outputs. Each rule at least must have a: Name: e.g., rule clean_data. Input: files or results needed. Output: files the step produces. Command: the command or script that performs the transformation. This rule, for example, outputs the row counts of a file to another file: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; 5.2.2.1 Name It must be unique, and ideally informative, as it will be used to identify the rules in the overview files, logs, etc. If you use repeated names, snakemake will complain and stop working. 5.2.2.2 Input and output Snakemake lets you describe input and output files in several convenient ways. 5.2.2.2.1 Single file rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; or rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; 5.2.2.2.2 Multiple unordered files rule count_lines: input: [&quot;results/a.counts.txt&quot;, &quot;results/b.counts.txt&quot;] output: &quot;results/merged.tsv&quot; shell: &quot;cat {input} &gt; {output}&quot; In this case, {input} expands to a space-separated list, this example resulting in: &quot;cat results/a.counts.txt results/b.counts.txt &gt; results/merged.tsv&quot; 5.2.2.2.3 Named children (recommended for clarity) rule count_lines: input: read1=&quot;reads/sample1_R1.fastq.gz&quot;, read2=&quot;reads/sample1_R2.fastq.gz&quot;, ref=&quot;refs/genome.fa&quot; output: bam=&quot;results/sample1.bam&quot; shell: &quot;bwa mem {input.ref} {input.read1} {input.read2} | &quot; &quot;samtools view -b -o {output.bam} 2&gt; {log.err}&quot; Use dot notation like {input.read1}, {output.bam}, etc. 5.2.2.3 Command Snakemake gives you several ways to express the command of a rule. 5.2.2.3.1 Shell The most common one is a regular shell command: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; The command definition can also be more complex, and expressed in multiple lines: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;&quot;&quot; coverm genome \\ -b {input} \\ -s _ \\ -m relative_abundance count \\ --min-covered-fraction 0 \\ &gt; {output} if [ $(stat -c &#39;%s&#39; {input}) -lt 1 ] then rm {input} fi &quot;&quot;&quot; 5.2.2.3.2 Script Or can run python scripts instead: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; script: &quot;scripts/normalize.py&quot; 5.2.2.3.3 Run Or an inline Python block: rule concat: input: [&quot;pieces/a.txt&quot;, &quot;pieces/b.txt&quot;] output: &quot;all.txt&quot; run: with open(output[0], &quot;w&quot;) as out: for p in input: out.write(open(p).read()) 5.2.2.4 Child arguments 5.3 Linking rules together One of Snakemake’s most powerful features is that you don’t need to manually tell it the order in which to run rules. Instead, Snakemake infers the correct sequence automatically by looking at: The output files of one rule The input files of another When the output of one rule matches the input of another, Snakemake creates a connection between them—this connection is a dependency. Let’s see an example: rule all: input: &quot;results/summary.txt&quot; rule preprocess: input: &quot;raw/data.csv&quot; output: &quot;processed/data_clean.csv&quot; shell: &quot;python scripts/clean_data.py {input} {output}&quot; rule summarize: input: &quot;processed/data_clean.csv&quot; output: &quot;results/summary.txt&quot; shell: &quot;python scripts/summarize.py {input} {output}&quot; 5.4 Executing the pipeline The execution of the pipeline is very straightforward. From the command line, in the directory containing your Snakefile, just run: snakemake By default, Snakemake looks for a file named Snakefile in the current directory, or in the workflow directory, both with and without a capital S. snakefile Snakefile workflow/snakefile workflow/Snakefile It is also possible to define a different name for the snakefile, usually with the extension .smk. If so, you need to tell snakemake where the snakefile is: snakemake -s mydirectory/mysnakefile.smk "],["course-environment.html", "Chapter 6 Course environment 6.1 Your directory 6.2 Raw data", " Chapter 6 Course environment The computational exercises of the course will be conducted in the following project in Mjolnir: /projects/course_1 You can access it using cd /projects/course_1 or visualise its contents using ls -lah /projects/course_1. This directory contains four folders: apps: it contains the snakemake pipelines you will use for processing the samples. data: it contains the raw data to be used in the exercises. people: it contains the folders of the individual users where your personal data will be stored. scratch: some software will use this directory to store temporary data. 6.1 Your directory This is the directory that can be accessed and edited by each user. All the computed files will be stored here. To access your directory you need to know your ku-id. /projects/course_1/people/{ku-id} 6.2 Raw data The raw data for the different studies that will be used in the course are found in different folders within the data director: /projects/course_1/data/lizards /projects/course_1/data/corvids "],["preprocessing.html", "Chapter 7 Preprocessing 7.1 Quality filtering 7.2 Host mapping 7.3 Host genome mapping 7.4 Mapping statistics 7.5 Host-microbiota split 7.6 Read-based profiling using SingleM 7.7 Metagenomic fraction estimation using SingleM prokaryotic fraction", " Chapter 7 Preprocessing Data preprocessing is a critical initial step in any host-microbiota multi-omics analysis workflow. This chapter details the Snakemake rules that perform essential preprocessing tasks, including quality filtering of raw sequencing reads, mapping to host reference genomes, and separating host-derived reads from microbial reads. Each rule is designed to ensure high data quality and integrity, setting the stage for accurate downstream analyses such as taxonomic profiling, functional annotation, and integrative multi-omics studies. 7.1 Quality filtering This rule performs the first major preprocessing step in the workflow: quality-control, trimming, and filtering of raw paired-end sequencing reads using fastp. It takes each sample’s raw FASTQ files and removes adapters, trims low-quality bases, filters out low-complexity or artifact-rich reads (including common NovaSeq poly-G/X issues), and outputs cleaned read pairs. These cleaned FASTQ files form the high-quality input for all downstream host–microbiota analyses, ensuring that later steps—such as host-read removal, taxonomic profiling, or assembly—are not biased by sequencing artifacts. In addition to generating cleaned reads, the rule produces HTML and JSON quality-control reports summarizing read quality before and after processing. The rule also handles software environment setup through module loading and scales memory/runtime requirements automatically based on the size of the input files, making it robust on HPC systems. rule fastp: input: r1=f&quot;{OUTPUT_DIR}/data/reads/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/data/reads/{{sample}}_2.fq.gz&quot; output: r1=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_2.fq.gz&quot;, html=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}.html&quot;, json=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}.json&quot; shell: &quot;&quot;&quot; module load {params.fastp_module} fastp \\ --in1 {input.r1} --in2 {input.r2} \\ --out1 {output.r1} --out2 {output.r2} \\ --trim_poly_g \\ --trim_poly_x \\ --low_complexity_filter \\ --n_base_limit 5 \\ --qualified_quality_phred 20 \\ --length_required 60 \\ --thread {threads} \\ --html {output.html} \\ --json {output.json} \\ --adapter_sequence AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\ --adapter_sequence_r2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT &quot;&quot;&quot; 7.2 Host mapping This rule prepares a reference genome so it can be efficiently used for read alignment with Bowtie2. It takes a FASTA file for a given host genome and runs bowtie2-build to generate the index files that Bowtie2 requires for fast and accurate mapping. Only one of the produced index files is listed as the output, but the command generates the full set of Bowtie2 index components. This indexing step is essential because alignment tools rely on these precomputed structures to rapidly search the genome during mapping. In addition to building the index, the rule also saves a copy of the reference FASTA alongside the index under a standardized naming scheme, ensuring consistency for downstream processes that expect the reference to be colocated with its index. The rule loads the appropriate Bowtie2 module and includes adaptive memory and runtime estimation based on the size of the reference genome, making it robust for small microbial genomes as well as larger host genomes. rule reference_index: input: f&quot;{OUTPUT_DIR}/data/references/{{reference}}.fna&quot; output: index=f&quot;{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2&quot; params: basename=f&quot;{OUTPUT_DIR}/data/references/{{reference}}&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} bowtie2-build {input} {params.basename} cat {input} &gt; {params.basename}.fna &quot;&quot;&quot; 7.3 Host genome mapping This rule maps each sample’s quality-filtered paired-end reads to its corresponding reference genome using Bowtie2, producing a sorted BAM file that is ready for downstream analyses such as host read removal, quantification, or coverage estimation. It uses the Bowtie2 index generated in the previous step and the cleaned FASTQ files from fastp as inputs. The mapping is performed with multiple threads for efficiency, and the output is streamed directly into samtools to convert the SAM alignment to BAM format and sort it in a single streamlined command. By linking each sample to its appropriate reference through SAMPLE_TO_REFERENCE, the rule flexibly supports host–microbiota workflows where different samples may require different reference genomes. Memory and runtime are scaled according to the size of the input reads, ensuring robust performance across varied dataset sizes. The result is a properly formatted, sorted BAM file that downstream tools can use without additional preprocessing. rule reference_map: input: index=lambda wildcards: expand( f&quot;{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2&quot;, reference=[SAMPLE_TO_REFERENCE[wildcards.sample]] ), r1=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/fastp/{{sample}}_2.fq.gz&quot; output: f&quot;{OUTPUT_DIR}/preprocessing/bowtie2/{{sample}}.bam&quot; params: basename=lambda wildcards: f&quot;{OUTPUT_DIR}/data/references/{SAMPLE_TO_REFERENCE[wildcards.sample]}&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} {params.samtools_module} bowtie2 -x {params.basename} -1 {input.r1} -2 {input.r2} -p {threads} | samtools view -bS - | samtools sort -o {output} &quot;&quot;&quot; 7.4 Mapping statistics This rule computes alignment quality and summary statistics for each sorted BAM file generated during mapping. Using several samtools subcommands, it produces metrics that describe how well the reads aligned to the reference genome—information essential for evaluating host-read removal efficiency, assessing sequencing quality, and feeding into MultiQC reports. The BAM file is indexed to enable rapid random access, while flagstat, idxstats, and stats each provide different levels of summary: overall alignment rates, per-reference sequence counts, and more detailed alignment metrics. All these outputs are written into a dedicated samtools directory and follow a consistent naming structure, making them easy to collect later for QC visualization and downstream processing. The rule loads the necessary samtools module and uses light computational resources, since generating these QC summaries is relatively inexpensive compared to alignment. rule samtools_stats: input: rules.reference_map.output output: bai = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.bam.bai&quot;, flagstat = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.flagstat.txt&quot;, idxstats = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.idxstats.txt&quot;, stats = f&quot;{OUTPUT_DIR}/preprocessing/samtools/{{sample}}.stats.txt&quot; shell: &quot;&quot;&quot; module load {params.samtools_module} samtools index {input} {output.bai} samtools flagstat {input} &gt; {output.flagstat} samtools idxstats {input} &gt; {output.idxstats} samtools stats {input} &gt; {output.stats} &quot;&quot;&quot; 7.5 Host-microbiota split This rule takes the aligned BAM file and separates mapped and unmapped read pairs, effectively splitting host-origin reads from metagenomic reads. Unmapped reads (i.e., those that did not align to the host reference) are extracted and converted back into paired FASTQ files, forming the metagenomic read set used for downstream microbiome profiling or assembly. At the same time, mapped reads are retained in a host-only BAM file, which can be used for host genomics analyses or simply for quality assessment. In addition to generating the split read files, the rule also counts how many reads and how many total bases fall into the metagenomic vs. host categories. These summary files are useful for evaluating host contamination, sequencing depth distribution, and the overall success of the host-depletion strategy. The process relies on samtools to filter BAM records by mapping flags and uses simple awk commands to tally base counts, making it a compact but crucial step in workflows that jointly analyze host and microbiota data. rule split_reads: input: f&quot;{OUTPUT_DIR}/preprocessing/bowtie2/{{sample}}.bam&quot; output: r1=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz&quot;, r2=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz&quot;, metareads=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.metareads&quot;, metabases=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.metabases&quot;, bam=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam&quot;, hostreads=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.hostreads&quot;, hostbases=f&quot;{OUTPUT_DIR}/preprocessing/final/{{sample}}.hostbases&quot; shell: &quot;&quot;&quot; module load {params.bowtie2_module} {params.samtools_module} samtools view -b -f12 -@ {threads} {input} | samtools fastq -@ {threads} -1 {output.r1} -2 {output.r2} - samtools view -b -f12 -@ {threads} {input} | samtools view -c - &gt; {output.metareads} samtools view -f12 -@ {threads} {input} | awk &#39;{{sum += length($10)}} END {{print sum}}&#39; &gt; {output.metabases} samtools view -b -F12 -@ {threads} {input} | samtools sort -@ {threads} -o {output.bam} - samtools view -b -F12 -@ {threads} {input} | samtools view -c - &gt; {output.hostreads} samtools view -F12 -@ {threads} {input} | awk &#39;{{sum += length($10)}} END {{print sum}}&#39; &gt; {output.hostbases} &quot;&quot;&quot; 7.6 Read-based profiling using SingleM This rule profiles the microbial community composition of metagenomic samples using SingleM, which targets single-copy marker genes to estimate taxonomic abundances. It takes the quality-filtered, host-depleted paired-end FASTQ files as input and runs the SingleM pipeline to generate a taxonomic profile for each sample. The output is a profile file that summarizes the relative abundances of microbial taxa present in the metagenomic dataset. rule singlem: input: r1=f&quot;{WORKDIR}/preprocessing/fastp_unique/{{sample}}_1.fq.gz&quot;, r2=f&quot;{WORKDIR}/preprocessing/fastp_unique/{{sample}}_2.fq.gz&quot; output: f&quot;{WORKDIR}/preprocessing/singlem/{{sample}}.profile&quot; params: siglemdir = f&quot;{WORKDIR}/preprocessing/singlem/&quot; shell: &quot;&quot;&quot; module load singlem/0.19.0 export SINGLEM_METAPACKAGE_PATH=/maps/datasets/globe_databases/singlem/5.4.0/S5.4.0.GTDB_r226.metapackage_20250331.smpkg.zb mkdir -p {params.siglemdir} singlem pipe \\ -1 {input.r1} \\ -2 {input.r2} \\ -p {output} &quot;&quot;&quot; 7.7 Metagenomic fraction estimation using SingleM prokaryotic fraction This rule estimates the microbial fraction of metagenomic samples using SingleM’s microbial_fraction command. It takes the quality-filtered paired-end FASTQ files and the SingleM taxonomic profile as inputs, and computes the proportion of reads that are of microbial origin. The output is a fraction file that quantifies the microbial content in each sample, which is useful for interpreting the assembly, binning and quantification results. rule spf: input: r1=f&quot;{WORKDIR}/preprocessing/fastp/{{sample}}_1.fq.gz&quot;, r2=f&quot;{WORKDIR}/preprocessing/fastp/{{sample}}_2.fq.gz&quot;, profile=f&quot;{WORKDIR}/preprocessing/singlem/{{sample}}.profile&quot; output: f&quot;{WORKDIR}/preprocessing/singlem/{{sample}}.fraction&quot; params: workdir = lambda wc: f&quot;{WORKDIR}/preprocessing/singlem/{wc.sample}&quot; threads: 1 shell: &quot;&quot;&quot; module load singlem/0.19.0 export SINGLEM_METAPACKAGE_PATH=/maps/datasets/globe_databases/singlem/5.4.0/S5.4.0.GTDB_r226.metapackage_20250331.smpkg.zb singlem microbial_fraction \\ -1 {input.r1} \\ -2 {input.r2} \\ -p {input.profile} &gt; {output} &quot;&quot;&quot; "],["host-genome.html", "Host genome 7.8 Extracting host reads from BAM files 7.9 Simplifying reads for single-end tools 7.10 Sketching host genome with Skmer 7.11 Calculating pairwise distances between host genomes 7.12 PCoA from distance matrix 7.13 Heatmap from distance matrix", " Host genome This section reconstructs host genomic relationships from low-coverage genome skimming data—enough breadth to capture species- and population-level signals without full assemblies. We extract host-aligned reads, optionally simplify to single-end for tools that require it, sketch genomes with Skmer, and derive pairwise distance matrices that feed into ordination and visualization. The core signal comes from k-mers (short subsequences of length k): their presence/absence patterns capture genome-wide similarities and differences, making them ideal for rapid, assembly-free comparison even when coverage is shallow. 7.8 Extracting host reads from BAM files Convert mapped BAMs to paired FASTQ to recover the host reads used for downstream genome sketching. rule bam_to_fastq: input: f&quot;{BAMS}/{{sample}}.bam&quot; output: r1=f&quot;{WORKDIR}/genomics/reads/{{sample}}_1.fq&quot;, r2=f&quot;{WORKDIR}/genomics/reads/{{sample}}_2.fq&quot; shell: &quot;&quot;&quot; module load samtools/1.21 samtools collate -u -O {input} | samtools fastq -1 {output.r1} -2 {output.r2} &quot;&quot;&quot; 7.9 Simplifying reads for single-end tools Some tools only accept single-end input; this step keeps R1, drops R2, and consolidates into one file. rule simplify_reads: input: r1=f&quot;{WORKDIR}/genomics/reads/{{sample}}_1.fq&quot;, r2=f&quot;{WORKDIR}/genomics/reads/{{sample}}_2.fq&quot; output: f&quot;{WORKDIR}/genomics/reads/{{sample}}/{{sample}}.fq&quot; localrule: True shell: &quot;&quot;&quot; rm {input.r2} mv {input.r1} {output} &quot;&quot;&quot; 7.10 Sketching host genome with Skmer Skmer creates compact k-mer sketches of each skimmed genome, retaining enough signal to approximate whole-genome distances while avoiding heavy assemblies. It hashes representative k-mers into a small index, making downstream pairwise comparisons fast and memory-light even when coverage is uneven. rule skmer_reference: input: f&quot;{WORKDIR}/genomics/reads/{{sample}}/{{sample}}.fq&quot; output: f&quot;{WORKDIR}/genomics/skmer/{{sample}}/{{sample}}.dat&quot; params: inputdir=f&quot;{WORKDIR}/genomics/reads/{{sample}}&quot;, outputbase=&quot;skmer&quot; shell: &quot;&quot;&quot; module load skmer/3.3.0 skmer reference {params.inputdir} -p {threads} -l {params.outputbase} &quot;&quot;&quot; 7.11 Calculating pairwise distances between host genomes Compute all-vs-all Skmer distances across samples, yielding a genome-wide dissimilarity matrix that reflects shared and unique k-mers from the skimmed reads. This captures broad phylogenetic structure and fine-scale population differences without requiring assemblies. rule skmer_distance: input: expand(f&quot;{WORKDIR}/genomics/skmer/{{sample}}/{{sample}}.dat&quot;, sample=SAMPLES) output: f&quot;{WORKDIR}/genomics/distance_matrix.txt&quot; params: inputdir=f&quot;{WORKDIR}/genomics/skmer&quot;, outputbase=&quot;distance_matrix&quot; shell: &quot;&quot;&quot; module load skmer/3.3.0 skmer distance {params.inputdir} -p {threads} -o {params.outputbase} &quot;&quot;&quot; 7.12 PCoA from distance matrix Project the distance matrix into principal coordinate space to visualize host genomic structure. rule skmer_pcoa: input: f&quot;{WORKDIR}/genomics/distance_matrix.txt&quot; output: f&quot;{WORKDIR}/genomics/pcoa.png&quot; params: outputbase=&quot;distance_matrix&quot; shell: &quot;&quot;&quot; python workflow/scripts/pcoa_from_distance.py --dist {input} --out-prefix pcoa &quot;&quot;&quot; 7.13 Heatmap from distance matrix Render the distance matrix as a clustered heatmap for quick inspection of host relatedness patterns. rule skmer_heatmap: input: f&quot;{WORKDIR}/genomics/distance_matrix.txt&quot; output: f&quot;{WORKDIR}/genomics/heatmap.png&quot; params: outputbase=&quot;distance_matrix&quot; shell: &quot;&quot;&quot; python workflow/scripts/heatmap_from_distance.py --dist {input} --out {output} &quot;&quot;&quot; "],["microbial-metagenome.html", "Microbial metagenome 7.14 Assembly with Megahit 7.15 Assembly index 7.16 Assembly mapping 7.17 Assembly map depth 7.18 Taxonomic annotation with GTDB-tk 7.19 Gene prediction with Prodigal 7.20 Functional annotation with KEGG hmms", " Microbial metagenome Microbial metagenomic assembly and binning are key steps in reconstructing the genomes of microorganisms present in complex communities. This chapter describes the Snakemake rules that perform metagenomic assembly using MEGAHIT, create Bowtie2 indices for the assemblies, map reads back to the assemblies, calculate contig coverage, and bin contigs into metagenome-assembled genomes (MAGs) using multiple binning tools (MetaBAT2, MaxBin2, SemiBin2). Finally, it covers the refinement of bins using Binette to produce high-quality MAGs for downstream analyses. 7.14 Assembly with Megahit This rule performs metagenomic assembly using MEGAHIT. For each defined assembly group, it gathers the metagenomic paired-end reads (R1 and R2) from one or more samples, combines them, and runs MEGAHIT to reconstruct longer contiguous sequences (contigs) from the short reads. This can be a single-sample assembly or a co-assembly of multiple related samples, depending on how ASSEMBLY_TO_SAMPLES is defined. The result is a FASTA file containing assembled contigs that represent fragments of microbial genomes present in the samples, filtered to retain only contigs above a certain minimum length to focus on more reliable sequences. The rule ensures that the output directory is clean before each run, uses multiple threads and dynamically allocated memory to handle larger datasets, and then standardizes the output by renaming MEGAHIT’s final contig file to a consistent path and filename. This assembled contig file becomes the basis for downstream cataloging steps such as binning, annotation, and construction of metagenome-assembled genomes (MAGs) in the host-microbiota multi-omics workflow. rule assembly: input: r1=f&quot;{READS}/{{sample}}_1.fq.gz&quot;, r2=f&quot;{READS}/{{sample}}_2.fq.gz&quot; output: f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}.fna&quot; params: outputdir=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}&quot; shell: &quot;&quot;&quot; module load megahit/1.2.9 rm -rf {params.outputdir} megahit \\ -t {threads} \\ --verbose \\ --min-contig-len 1500 \\ -1 {input.r1} -2 {input.r2} \\ -o {params.outputdir} mv {params.outputdir}/final.contigs.fa {output} &quot;&quot;&quot; 7.15 Assembly index This rule creates a Bowtie2 index for each metagenomic assembly generated in the previous step. It takes the assembled contigs (the reconstructed metagenome) and runs bowtie2-build to generate the indexing files required for efficient read alignment. Indexing metagenomic assemblies is essential for downstream processes such as read recruitment, contig coverage estimation, binning, and quality assessment of assembled genomes. The rule ensures the index is stored alongside the assembly under a consistent naming scheme, allowing later rules to easily locate it. It uses modest computational resources, since indexing contigs is generally lightweight compared to assembly, and loads the correct Bowtie2 module to guarantee reproducibility. This step prepares the assembled contigs to be used as a searchable reference in subsequent cataloging analyses. rule assembly_index: input: f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}.fna&quot; output: index=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}.rev.2.bt2&quot; params: basename=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}&quot; shell: &quot;&quot;&quot; module load bowtie2/2.4.2 bowtie2-build {input} {params.basename} &quot;&quot;&quot; 7.16 Assembly mapping This rule maps each sample’s metagenomic reads back to a specific metagenomic assembly using Bowtie2. By aligning the cleaned, host-filtered reads to the assembled contigs, the workflow can quantify how much support each contig has across samples—information that is essential for downstream steps such as binning, abundance estimation, contamination checking, and cross-sample comparative analyses. The rule uses the Bowtie2 index generated for the assembly and the final FASTQ files produced during host–microbiota read separation. The alignment output is directly converted to a sorted BAM file through samtools, ensuring the result is immediately usable for coverage calculation or other depth-based analyses. With multi-threading support and adaptive resource allocation, the rule efficiently handles both small and large datasets. This mapping step links reads back to the assembly, turning raw contigs into quantitatively interpretable components of the microbial community. rule assembly_map: input: index=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}.rev.2.bt2&quot;, r1=f&quot;{READS}/{{sample}}_1.fq.gz&quot;, r2=f&quot;{READS}/{{sample}}_2.fq.gz&quot; output: f&quot;{WORKDIR}/metagenomics/bowtie2/{{sample}}.bam&quot; params: basename=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}&quot; shell: &quot;&quot;&quot; module load bowtie2/2.4.2 samtools/1.21 bowtie2 -x {params.basename} -1 {input.r1} -2 {input.r2} | samtools view -bS - | samtools sort -o {output} &quot;&quot;&quot; 7.17 Assembly map depth This rule computes per-contig read depth across all samples associated with a given assembly. It takes the set of BAM files produced by mapping each sample’s metagenomic reads back to the assembly and uses jgi_summarize_bam_contig_depths (from MetaBAT2) to generate a depth table. This table reports how many reads align to each contig in each sample—information that is essential for binning tools, which rely on differential coverage patterns to group contigs into metagenome-assembled genomes (MAGs). The rule outputs two versions of the depth file: the full MetaBAT2-compatible depth matrix and a simplified depth file formatted for MaxBin2. These coverage profiles are a key input for downstream binning workflows, as they allow algorithms to distinguish which contigs likely originate from the same genome. rule assembly_map_depth: input: f&quot;{WORKDIR}/metagenomics/bowtie2/{{sample}}.bam&quot; output: metabat2=f&quot;{WORKDIR}/metagenomics/bowtie2/{{sample}}_metabat.depth&quot;, maxbin2=f&quot;{WORKDIR}/metagenomics/bowtie2/{{sample}}_maxbin.depth&quot; shell: &quot;&quot;&quot; module load metabat2/2.17 jgi_summarize_bam_contig_depths --outputDepth {output.metabat2} {input} cut -f1,3 {output.metabat2} | tail -n+2 &gt; {output.maxbin2} &quot;&quot;&quot; 7.17.1 Binning with MetaBat2 MetaBAT2 clusters contigs based mainly on differential coverage across samples and tetranucleotide frequency, producing a TSV mapping contigs to bins. It is fast, widely used, and effective when multiple samples provide rich coverage variation. rule metabat2: input: assembly=f&quot;{WORKDIR}/metagenomics/megahit/{{sample}}.fna&quot;, depth=f&quot;{WORKDIR}/metagenomics/bowtie2/{{sample}}_metabat.depth&quot; output: f&quot;{WORKDIR}/metagenomics/metabat2/{{sample}}.tsv&quot; params: basename=f&quot;{WORKDIR}/metagenomics/metabat2/{{sample}}/{{sample}}&quot; shell: &quot;&quot;&quot; module load metabat2/2.17 metabat2 -i {input.assembly} -a {input.depth} -o {params.basename} -m 1500 --saveCls # Generate summary file for dRep find &quot;$(dirname {params.basename})&quot; -maxdepth 1 -type f -name &quot;*$(basename {params.basename})_*.fa&quot; | sort &gt; {output} &quot;&quot;&quot; 7.17.2 Bin quality with CheckM CheckM2 estimates each bin’s completeness and contamination by comparing marker genes against curated reference profiles. This step provides an objective measure of MAG quality before downstream analyses. The rule below takes the MetaBAT2 bins for each assembly, runs CheckM2, and writes both a detailed quality_report.tsv and a simplified CSV with just genome name, completeness, and contamination. That CSV is formatted so dRep can quickly apply quality filters during dereplication. rule metabat2: input: f&quot;{WORKDIR}/metagenomics/metabat2/{{sample}}.tsv&quot; output: f&quot;{WORKDIR}/metagenomics/checkm2/{{sample}}.tsv&quot; params: bins_dir=lambda wildcards: f&quot;{WORKDIR}/metagenomics/metabat2/{wildcards.sample}&quot;, outdir=f&quot;{WORKDIR}/metagenomics/checkm2/{{sample}}&quot; shell: &quot;&quot;&quot; module load checkm2/1.0.2 rm -rf {params.outdir} mkdir -p {params.outdir} checkm2 predict -i {params.bins_dir}/*.fa -o {params.outdir} -t {threads} --database_path /maps/datasets/globe_databases/checkm2/20250215/CheckM2_database/uniref100.KO.1.dmnd # Prepare genome info for drep awk -F&#39;\\t&#39; &#39;BEGIN{{OFS=&quot;,&quot;}} NR==1{{print &quot;genome&quot;,&quot;completeness&quot;,&quot;contamination&quot;; next}} {{print $1&quot;.fna&quot;,$2,$3}}&#39; {params.outdir}/quality_report.tsv &gt; {output} &quot;&quot;&quot; 7.17.3 Bin dereplication with dRep dRep clusters near-identical MAGs and retains the highest-quality representative from each cluster, reducing redundancy before annotation or comparative genomics. Using the completeness and contamination estimates from CheckM2, it filters low-quality bins, computes pairwise ANI with FastANI/Mash, and outputs the selected genomes plus summary tables. The rule consumes the MetaBAT2 bin list and the CheckM2 quality table, then writes dereplication outputs to a dedicated directory for each assembly. rule metabat2: input: genomes=f&quot;{WORKDIR}/metagenomics/metabat2/{{sample}}.tsv&quot;, genomeinfo=f&quot;{WORKDIR}/metagenomics/checkm2/{{sample}}.tsv&quot; output: f&quot;{WORKDIR}/metagenomics/drep/{{sample}}/data_tables/genomeInformation.csv&quot; params: bins_dir=lambda wildcards: f&quot;{WORKDIR}/metagenomics/metabat2/{wildcards.sample}&quot;, outdir=f&quot;{WORKDIR}/metagenomics/drep/{{sample}}&quot; shell: &quot;&quot;&quot; module load drep/3.6.2 fastani/1.33 mash/2.3 rm -rf {params.outdir} dRep dereplicate {params.outdir} -g {input.genomes} -p {threads} -pa 0.95 --genomeInfo {input.genomeinfo} &quot;&quot;&quot; 7.18 Taxonomic annotation with GTDB-tk GTDB-Tk classifies each dereplicated MAG against the curated Genome Taxonomy Database, providing standardized domain-to-species labels and quality-controlled marker gene checks. Adding taxonomy here anchors downstream interpretations—linking functional profiles, differential abundance, or host associations to known clades and flagging potential contaminants or mis-binned genomes. The rule will take the dereplicated bins, run GTDB-Tk, and emit per-genome classifications plus summary reports for quick review. rule gtdbtk: output: f&quot;{WORKDIR}/metagenomics/gtdbtk/classify/gtdbtk.bac120.summary.tsv&quot; params: bindir=BINDIR, outdir=f&quot;{WORKDIR}/metagenomics/gtdbtk&quot; shell: &quot;&quot;&quot; export GTDBTK_DATA_PATH=/datasets/globe_databases/gtdbtk_db/20241001 gtdbtk classify_wf \\ --genome_dir {params.bindir} \\ --out_dir {params.outdir} \\ --cpus {threads} \\ --extension fa \\ --skip_ani_screen 7.19 Gene prediction with Prodigal This rule predicts genes in dereplicated MAGs using Prodigal, generating GFF annotations, nucleotide sequences, and amino acid sequences. Gene prediction is a crucial step for functional annotation, as it identifies coding regions within the MAGs that can be further analyzed for their potential roles in microbial metabolism and ecology. rule prodigal: input: f&quot;{BINDIR}/{{mag}}.fa&quot; output: gff=f&quot;{WORKDIR}/metagenomics/prodigal/{{mag}}.gff&quot;, nt=f&quot;{WORKDIR}/metagenomics/prodigal/{{mag}}.fna&quot;, aa=f&quot;{WORKDIR}/metagenomics/prodigal/{{mag}}.faa&quot; shell: &quot;&quot;&quot;module mkdir -p $(dirname {output.gff}) module load pprodigal/1.0.1 prodigal -i {input} -o {output.gff} -d {output.nt} -a {output.aa} -p single &quot;&quot;&quot; 7.20 Functional annotation with KEGG hmms This rule annotates dereplicated MAGs using KEGG HMM profiles to assign functional roles based on gene content. It takes the predicted amino acid sequences from Prodigal and scans them against the KEGG Orthology HMM database using HMMER’s hmmscan. The output includes both a detailed text file and a tab-separated values (TSV) file summarizing the functional annotations for each gene. This functional annotation step is essential for understanding the metabolic capabilities of the reconstructed genomes and their potential roles within microbial communities. rule kegg: input: f&quot;{WORKDIR}/metagenomics/prodigal/{{mag}}.faa&quot; output: txt=f&quot;{WORKDIR}/metagenomics/kegg/{{mag}}.txt&quot;, tsv=f&quot;{WORKDIR}/metagenomics/kegg/{{mag}}.tsv&quot; params: db=&quot;/projects/alberdilab/data/databases/drakkar/kofams&quot; shell: &quot;&quot;&quot; module load hmmer/3.3.2 hmmscan -o {output.txt} --tblout {output.tsv} -E 1e-10 --noali {params.db} {input} &quot;&quot;&quot; "],["metatranscriptome.html", "Meta(transcriptome)", " Meta(transcriptome) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
